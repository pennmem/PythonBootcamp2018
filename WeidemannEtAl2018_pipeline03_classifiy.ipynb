{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import multiprocessing as mp\n",
    "from glob import glob\n",
    "# from sklearn import cross_validation\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import pickle\n",
    "\n",
    "\n",
    "# if > 0 use multiprocessing, otherwise regular single-process\n",
    "# execution:\n",
    "\n",
    "run_on_rhino = True\n",
    "# run_on_rhino = False\n",
    "# experiment = 'FR1'\n",
    "experiment = 'catFR1'\n",
    "\n",
    "GOOD_ELECS_ONLY = False\n",
    "# GOOD_ELECS_ONLY = True\n",
    "\n",
    "if run_on_rhino:\n",
    "    rhino_mount = ''\n",
    "else:\n",
    "    rhino_mount = '/home/ctw/fusemounts/rhino'\n",
    "\n",
    "\n",
    "\n",
    "if GOOD_ELECS_ONLY:\n",
    "    class_path_suffix = '_goodchans'\n",
    "else:\n",
    "    class_path_suffix = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root = '/scratch/cweidema/bootcamp/tmpdat'\n",
    "paths = {\n",
    "    'power': {\n",
    "        'catFR1': (rhino_mount + output_root +\n",
    "                   '/RAM/RAM_catFR/' +\n",
    "                   'RAM_catFR1_power/encoding/hdf5_files_subj/'),\n",
    "        'FR1': (rhino_mount + output_root +\n",
    "                '/RAM/RAM_FR/' +\n",
    "                'RAM_FR1_power/encoding/hdf5_files_subj/')},\n",
    "    'classifier': {\n",
    "        'catFR1': (rhino_mount + output_root +\n",
    "                   '/RAM/RAM_FRcatFR/encoding'+\n",
    "                   class_path_suffix+'/' +\n",
    "                   'RAM_catFR1lolo_classifiers_allfreqs/'+\n",
    "                   'RAM_catFR1lolo_classifiers_allfreqs_withshuffles/'),\n",
    "    'FR1': (rhino_mount + output_root +\n",
    "            '/RAM/RAM_FRcatFR/encoding'+\n",
    "            class_path_suffix+'/' +\n",
    "            'RAM_FR1lolo_classifiers_allfreqs/'+\n",
    "            'RAM_FR1lolo_classifiers_allfreqs_withshuffles/'),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# power_files_all = {}\n",
    "# for exp in paths['power']:\n",
    "#     power_files_all[exp] = np.sort(glob(os.path.join(\n",
    "#         paths['power'][exp], '*_pow.hdf5')))\n",
    "\n",
    "# power_files_bothexp = {}\n",
    "# for exp in power_files_all:\n",
    "#     power_files_bothexp[exp] = []\n",
    "\n",
    "# for pf in power_files_all['catFR1']:\n",
    "#     pf_FR = pf.replace('catFR', 'FR')\n",
    "#     if pf_FR in power_files_all['FR1']:\n",
    "#         power_files_bothexp['catFR1'].append(pf)\n",
    "#         power_files_bothexp['FR1'].append(pf_FR)\n",
    "\n",
    "exp = 'catFR1'\n",
    "power_files_bothexp = {exp: np.sort(\n",
    "    glob(os.path.join(\n",
    "        paths['power'][exp], '*_pow.hdf5')))}\n",
    "        \n",
    "        \n",
    "classifier_path = paths['classifier'][experiment]\n",
    "subjpaths = power_files_bothexp[experiment]\n",
    "\n",
    "time_bins = [(0.0, 1.6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [np.logspace(np.log10(1e-6), np.log10(1e4), 22)[6]]\n",
    "\n",
    "lr_params = {'penalty': 'l2', 'dual': False, 'tol': 0.0001, 'C': np.nan,\n",
    "             'fit_intercept': True, 'intercept_scaling': 1,\n",
    "             'class_weight': 'balanced', 'random_state': None,\n",
    "             'solver': 'liblinear', 'max_iter': 2000,\n",
    "             'multi_class': 'ovr', 'verbose': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_lr(lr_params, features, target, train, test, indx=-1):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression(**lr_params).fit(\n",
    "        features[train], target[train])\n",
    "    classifier_info = {\n",
    "        'trained_classifier': lr,\n",
    "        'train': train,\n",
    "        'test': test,\n",
    "        'decision_function': lr.decision_function(features[test]),\n",
    "        'predict': lr.predict(features[test]),\n",
    "        'log_probas': lr.predict_log_proba(features[test]),\n",
    "        'probas': lr.predict_proba(features[test]),\n",
    "        'score': lr.score(features[test], target[test]),\n",
    "        'params': lr.get_params(),\n",
    "        'coef': lr.coef_,\n",
    "        'intercept': lr.intercept_,\n",
    "        'indx': indx}\n",
    "    return(classifier_info)\n",
    "\n",
    "\n",
    "def train_test_lr_shuffles(lr_params, features, target, train, test,\n",
    "                           shuffles=1000, indx=-1):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    shuffled_classifiers = []\n",
    "    shuffled_classifier_info = []\n",
    "    for i in range(shuffles):\n",
    "        trgt = np.copy(target[train])\n",
    "        np.random.shuffle(trgt)\n",
    "        lr = LogisticRegression(**lr_params).fit(\n",
    "            features[train], trgt)\n",
    "        classifier_info = {\n",
    "            'trained_classifier': lr,\n",
    "            'train': train,\n",
    "            'test': test,\n",
    "            'decision_function': lr.decision_function(features[test]),\n",
    "            'predict': lr.predict(features[test]),\n",
    "            'log_probas': lr.predict_log_proba(features[test]),\n",
    "            'probas': lr.predict_proba(features[test]),\n",
    "            'score': lr.score(features[test], target[test]),\n",
    "            'params': lr.get_params(),\n",
    "            'coef': lr.coef_,\n",
    "            'intercept': lr.intercept_,\n",
    "            'train_target': trgt,\n",
    "            'indx': indx}\n",
    "        shuffled_classifiers.append(lr)\n",
    "        shuffled_classifier_info.append(classifier_info)\n",
    "    return(shuffled_classifiers, shuffled_classifier_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valchans(subjdat):\n",
    "    subj = subjdat['data'].attrs['subject']\n",
    "    elfile = glob(rhino_mount+'/data/eeg/'+subj+\n",
    "                  '*/docs/*electrode_categories.txt')\n",
    "    if len(elfile) == 0:\n",
    "        elfile = glob(rhino_mount+'/scratch/pwanda/electrode_categories/'+\n",
    "                      subj+'*_electrode_categories.txt')\n",
    "    if len(elfile) == 0:\n",
    "        elfile = glob(rhino_mount+'/scratch/pwanda/electrode_categories/' +\n",
    "                      'electrode_categories_' + subj + '*.txt')\n",
    "    if len(elfile) == 2:\n",
    "        subj_with_duplicates = ['R1171M', 'R1191J', 'R1290M', 'R1288P',\n",
    "                                'R1230J']\n",
    "        if subj in subj_with_duplicates:\n",
    "            elfile.pop(0)\n",
    "    if len(elfile) != 1:\n",
    "        raise ValueError('Invalid elfile for subj '+subj+': '+str(elfile))\n",
    "    elfile = elfile[0]\n",
    "    with open(elfile, 'r') as ef:\n",
    "        lines = [mystr.strip().replace(' ', '').upper()\n",
    "                 for mystr in ef.readlines()\n",
    "                 if len(mystr.strip().replace(' ', ''))> 0]\n",
    "    if 'tal_struct' in subjdat.keys():\n",
    "        bppairs = [bp.strip().replace(' ', '').upper().split('-')\n",
    "                   for bp in subjdat['tal_struct/code']]\n",
    "    elif 'h5info' in subjdat.keys():\n",
    "        bppairs = [bp.strip().replace(' ', '').upper().split('-')\n",
    "                   for bp in subjdat['h5info/bipolar_info/contact_name']]\n",
    "    else:\n",
    "        raise ValueError('Missing electrode info for '+str(subjdat))\n",
    "    badchans = np.array([np.any([e in lines for e in bp])\n",
    "                         for bp in bppairs])\n",
    "    return(~badchans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data, time_bins=time_bins, sessfilt=None,\n",
    "                 freqsfilt=None, channels=None, valid_chan_filter=None):\n",
    "    # valid_chan_filter = ~np.all(~np.isfinite(data[:, 0, :, 0]), 1)\n",
    "    if valid_chan_filter is None:\n",
    "        valid_chan_filter = ~np.all(~np.isfinite(data[0, :, :, 0]), -1)\n",
    "    else:\n",
    "        valid_chan_filter &= ~np.all(~np.isfinite(data[0, :, :, 0]), -1)\n",
    "    # valid_evs_filter = np.isfinite(np.min(data[valid_chan_filter, 0, :, 0], 0))\n",
    "    valid_evs_filter = np.isfinite(np.min(data[0, valid_chan_filter, :, 0], 0))\n",
    "    # make sure to delete complete lists:\n",
    "    invalid_sess_list = [(s, l) for s, l in zip(\n",
    "        data.dims[2]['session'][~valid_evs_filter],\n",
    "        data.dims[2]['list'][~valid_evs_filter])]\n",
    "    # it's a bit inefficient to loop through all of the elements of\n",
    "    # the list but creating a list of unique (session, list) tuples is\n",
    "    # a bit tricky in a way that works generally is a bit tricky\n",
    "    # (tried with np.unique and set) and in generally the\n",
    "    # invalid_sess_list list should be short:\n",
    "    for inval_sess, inval_list in invalid_sess_list:\n",
    "        valid_evs_filter[(data.dims[2]['session'][:] == inval_sess) &\n",
    "                         (data.dims[2]['list'][:] == inval_list)] = False\n",
    "    # data_valid = data_valid[:, :, valid_evs_filter]\n",
    "    if freqsfilt is None:\n",
    "        # freqsfilt = np.ones(data.shape[1], np.bool)\n",
    "        freqsfilt = np.ones(data.shape[0], np.bool)\n",
    "    # timebin_features = (len(data.dims[0]['channels'][valid_chan_filter]) * np.sum(freqsfilt))\n",
    "    if sessfilt is not None:\n",
    "        valid_evs_filter &= sessfilt\n",
    "    timebin_features = np.sum(valid_chan_filter) * np.sum(freqsfilt)\n",
    "\n",
    "    # if sessfilt is None:\n",
    "    #     # sessfilt = np.ones(data.shape[2], np.bool)\n",
    "    #     sessfilt = valid_evs_filter\n",
    "    # else:\n",
    "    #     sessfilt = valid_evs_filter & sessfilt\n",
    "    sessdat = data[:, :, valid_evs_filter]\n",
    "    # sessdat = sessdat[valid_chan_filter]\n",
    "    sessdat = sessdat[:, valid_chan_filter]\n",
    "    # sessdat = sessdat[:, freqsfilt]\n",
    "    sessdat = sessdat[freqsfilt]\n",
    "    features = {}\n",
    "    # features['data'] = np.ones((len(data.dims[2]['recalled']),\n",
    "    features['data'] = np.ones((np.sum(valid_evs_filter),\n",
    "                                timebin_features * len(time_bins))) * np.nan\n",
    "    features['time_bin_labels'] = []\n",
    "    features['channels'] = []\n",
    "    features['freqs'] = []\n",
    "    if channels is None:\n",
    "        channels = data.dims[0][0]\n",
    "    for t, tb in enumerate(time_bins):\n",
    "        time_filter = ((data.dims[3]['time'].value >= tb[0]) &\n",
    "                       (data.dims[3]['time'].value < tb[1]))\n",
    "        features['data'][\n",
    "            :, (t * timebin_features):((t + 1) * timebin_features)] = np.mean(\n",
    "            sessdat[:, :, :, time_filter], 3).reshape(\n",
    "                (np.sum(valid_chan_filter) *\n",
    "                 np.sum(freqsfilt),\n",
    "                 # len(data.dims[1]['freqs']),\n",
    "                 np.sum(valid_evs_filter))).T\n",
    "        #          len(data.dims[2]['recalled']))).T\n",
    "        features['time_bin_labels'].extend(\n",
    "            [str(tb[0]) + '-' + str(tb[1])] * timebin_features)\n",
    "        for chan in channels[valid_chan_filter]:\n",
    "            features['channels'].extend(\n",
    "                # [chan] * (len(data.dims[1]['freqs'])))\n",
    "                [chan] * (np.sum(freqsfilt)))\n",
    "            features['freqs'].extend(data.dims[1][0][freqsfilt])\n",
    "    return(features, valid_chan_filter, valid_evs_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification(subjpath, train_test_func, train_test_func_shuffles,\n",
    "                       classifier_params,\n",
    "                       classifier_path, time_bins, shuffles=1000):\n",
    "    classifier_path_pickle = classifier_path + 'pickle_files/'\n",
    "    classifier_path_hdf5 = classifier_path + 'hdf5_files/'\n",
    "    if not os.path.exists(classifier_path_pickle +\n",
    "                          str(classifier_params['C']) + '/'):\n",
    "        try:\n",
    "            os.makedirs(classifier_path_pickle+\n",
    "                        str(classifier_params['C']) + '/')\n",
    "        except OSError as e:\n",
    "            print('no problem', e)\n",
    "    if not os.path.exists(classifier_path_hdf5 +\n",
    "                          str(classifier_params['C']) + '/'):\n",
    "        try:\n",
    "            os.makedirs(classifier_path_hdf5 +\n",
    "                        str(classifier_params['C']) + '/')\n",
    "        except OSError as e:\n",
    "            print('no problem', e)\n",
    "    subjdat = h5py.File(subjpath, 'r')\n",
    "    if 'session' in subjdat['data'].attrs:\n",
    "        session = subjdat['data'].attrs['session']\n",
    "        out_path_pickle = str(\n",
    "            classifier_path_pickle + str(classifier_params['C']) +\n",
    "            '/' + str(subjdat['data'].attrs['subject']) +\n",
    "            '_' + str(subjdat['data'].attrs['session']) +\n",
    "            '_classifier_info.pickle')\n",
    "        out_path_hdf5 = str(classifier_path_hdf5 + str(classifier_params['C']) +\n",
    "                         '/' + str(subjdat['data'].attrs['subject']) +\n",
    "                         '_' + str(subjdat['data'].attrs['session']) +\n",
    "                         '_classifier_info.hdf5')\n",
    "    else:\n",
    "        out_path_pickle = str(\n",
    "            classifier_path_pickle + str(classifier_params['C']) +\n",
    "            '/' + str(subjdat['data'].attrs['subject']) + \n",
    "            '_classifier_info.pickle')\n",
    "        out_path_hdf5 = str(\n",
    "            classifier_path_hdf5 + str(classifier_params['C']) + '/' +\n",
    "            str(subjdat['data'].attrs['subject']) + \n",
    "            '_classifier_info.hdf5')\n",
    "    if os.path.exists(out_path_pickle) or os.path.exists(out_path_hdf5):\n",
    "        return\n",
    "    try:\n",
    "        out_hdf5 = h5py.File(out_path_hdf5, 'w-', libver='latest')\n",
    "    except IOError:\n",
    "        print('Cannot create', out_path_hdf5)\n",
    "        return\n",
    "    sess_list_labels = np.array(['{:02d}'.format(s)+'-'+'{:02d}'.format(l)\n",
    "                                 for s,l in zip(\n",
    "                                         subjdat['data'].dims[2]['session'][:],\n",
    "                                         subjdat['data'].dims[2]['list'][:])])\n",
    "    sessfilt = (np.array([np.sum((subjdat['data'].dims[2]['session'][:] == s) &\n",
    "                                (subjdat['data'].dims[2]['list'][:] == l))\n",
    "                         for s,l in zip(subjdat['data'].dims[2]['session'][:],\n",
    "                                        subjdat['data'].dims[2]['list'][:])])\n",
    "                == 12)\n",
    "    # remove practice lists:\n",
    "    sessfilt &= subjdat['data'].dims[2]['session'][:] >= 0\n",
    "    # sess_list_labels = sess_list_labels[sessfilt]\n",
    "    # sess_list_labels = sess_list_labels\n",
    "    logo = LeaveOneGroupOut()\n",
    "    # lolo = cross_validation.LeaveOneLabelOut(sess_list_labels)\n",
    "    # recalled = subjdat['data'].dims[2]['recalled'][sessfilt] == 1\n",
    "    recalled = subjdat['data'].dims[2]['recalled'][:] == 1\n",
    "    res = {}\n",
    "    res_shuffles = {}\n",
    "    means = {}\n",
    "    stds = {}\n",
    "    if GOOD_ELECS_ONLY:\n",
    "        valid_chan_filter = get_valchans(subjdat)\n",
    "    else:\n",
    "        valid_chan_filter = None\n",
    "    chans = None\n",
    "    if 'h5info' in subjdat.keys():\n",
    "        if 'bipolar_info' in subjdat['h5info'].keys():\n",
    "            chans = subjdat['h5info/bipolar_info/contact_name'][:]\n",
    "    elif 'tal_struct' in subjdat.keys():\n",
    "        # if 'code' in subjdat['tal_struct'].keys():           \n",
    "        chans = subjdat['tal_struct/code'][:]\n",
    "    for tb in time_bins:\n",
    "        means[tb] = []\n",
    "        stds[tb] = []\n",
    "        res[tb] = []\n",
    "        res_shuffles[tb] = []\n",
    "        features, valid_chan_filter, valid_evs_filter = get_features(\n",
    "            subjdat['data'], time_bins=[tb], sessfilt=sessfilt,\n",
    "            # freqsfilt=subjdat['freqs'][:] > 9)\n",
    "            freqsfilt=subjdat['freqs'][:] > 0, channels=chans,\n",
    "            valid_chan_filter=valid_chan_filter)\n",
    "        # for study_list, (train, test) in enumerate(lolo):\n",
    "        for study_list, (train, test) in enumerate(\n",
    "                logo.split(features['data'], recalled[valid_evs_filter],\n",
    "                                  groups=sess_list_labels[valid_evs_filter])):\n",
    "            print(study_list, '/', logo.get_n_splits(\n",
    "                features['data'], recalled[valid_evs_filter],\n",
    "                groups=sess_list_labels[valid_evs_filter]))\n",
    "            features_norm = features['data'].copy()\n",
    "            means[tb].append([])\n",
    "            stds[tb].append([])\n",
    "            assert(np.all([sess in np.unique(subjdat['events/session'][\n",
    "                valid_evs_filter][train])\n",
    "                           for sess in np.unique(subjdat['events/session'][\n",
    "                                   valid_evs_filter][test])]))\n",
    "            for sess in np.unique(\n",
    "                    subjdat['events/session'][valid_evs_filter]):\n",
    "                scalesessfilt = subjdat['events/session'][valid_evs_filter] == sess\n",
    "                sessmeans = features_norm[train][scalesessfilt[train]].mean(0)\n",
    "                sessstds = features_norm[train][scalesessfilt[train]].std(0)\n",
    "                means[tb][-1].append(sessmeans)\n",
    "                stds[tb][-1].append(sessstds)\n",
    "                features_norm[scalesessfilt] -= sessmeans\n",
    "                features_norm[scalesessfilt] /= sessstds\n",
    "            res[tb].append(\n",
    "                train_test_func(\n",
    "                    classifier_params, features_norm,\n",
    "                    recalled[valid_evs_filter], train,\n",
    "                    test, study_list))\n",
    "            res_shuffles[tb].append(\n",
    "                train_test_func_shuffles(\n",
    "                    classifier_params, features_norm,\n",
    "                    recalled[valid_evs_filter], train,\n",
    "                    test, shuffles, study_list))\n",
    "    try:\n",
    "        pickle.dump([res, res_shuffles], open(out_path_pickle, 'w'), -1)\n",
    "        # make file read only to avoid accidental loss:\n",
    "        os.chmod(out_path_pickle, 0o444)\n",
    "    except (IOError, TypeError):\n",
    "        print('Cannot create ', out_path_pickle)\n",
    "    features_out = out_hdf5.create_dataset(\n",
    "        'features', data=features['data'],\n",
    "        compression='gzip', compression_opts=9)\n",
    "    features_out.dims[0].label = 'events'\n",
    "    # out_hdf5.copy(subjdat['events'], out_hdf5, name='events')\n",
    "    out_hdf5.copy(subjdat['events'], out_hdf5, name='events')\n",
    "    for key in out_hdf5['events'].keys():\n",
    "        features_out.dims.create_scale(out_hdf5['events/' + key], key)\n",
    "        features_out.dims[0].attach_scale(out_hdf5['events/' + key])\n",
    "    # create dimension for features:\n",
    "    if 'tal_struct' in subjdat.keys():\n",
    "        out_hdf5.copy(subjdat['tal_struct'], out_hdf5, name='tal_struct')\n",
    "    if 'h5info' in subjdat.keys():\n",
    "        out_hdf5.copy(subjdat['h5info'], out_hdf5, name='h5info')\n",
    "    recalled = out_hdf5.create_dataset(\n",
    "        'target', data=recalled, compression=\"gzip\", compression_opts=9)\n",
    "    out_hdf5['channels'] = features['channels']\n",
    "    out_hdf5['valid_chan_filter'] = valid_chan_filter\n",
    "    out_hdf5['valid_evs_filter'] = valid_evs_filter    \n",
    "    out_hdf5['freqs'] = features['freqs']\n",
    "    # out_hdf5['time_bins'] = features['time_bin_labels']\n",
    "    out_hdf5['time_bins'] = time_bins\n",
    "    features_out.dims[1].label = 'time_bins*channels*frequencies'\n",
    "    features_out.dims.create_scale(out_hdf5['channels'], 'channels')\n",
    "    features_out.dims[1].attach_scale(out_hdf5['channels'])\n",
    "    features_out.dims.create_scale(out_hdf5['freqs'], 'freqs')\n",
    "    features_out.dims[1].attach_scale(out_hdf5['freqs'])\n",
    "    # features_out.dims.create_scale(out_hdf5['time_bins'], 'time_bins')\n",
    "    # features_out.dims[1].attach_scale(out_hdf5['time_bins'])\n",
    "    #\n",
    "    for param in classifier_params:\n",
    "        if param == 'random_state':\n",
    "            features_out.attrs[param] = str(classifier_params[param])\n",
    "        else:\n",
    "            features_out.attrs[param] = classifier_params[param]\n",
    "    for c_info in res[time_bins[0]][0]:\n",
    "        if c_info == 'trained_classifier':\n",
    "            continue\n",
    "        elif c_info == 'params':\n",
    "            for param in res[time_bins[0]][0][c_info]:\n",
    "                if param == 'random_state':\n",
    "                    features_out.attrs[param] = str(\n",
    "                        res[time_bins[0]][0][c_info][param])\n",
    "                else:\n",
    "                    features_out.attrs[param] = res[\n",
    "                        time_bins[0]][0][c_info][param]\n",
    "            continue\n",
    "        out_hdf5.create_dataset(\n",
    "            c_info, data = [[r[c_info] for r in res[tb]] for tb in time_bins],\n",
    "            compression=\"gzip\", compression_opts=9)\n",
    "        out_hdf5.create_dataset(\n",
    "            c_info + '_shuffles',\n",
    "            data = [[[r[c_info] for r in rtb[1]] for rtb in res_shuffles[tb]]\n",
    "                    for tb in time_bins],\n",
    "            compression=\"gzip\", compression_opts=9)\n",
    "    for tb in time_bins:\n",
    "        out_hdf5.create_dataset('means/'+str(tb), data=means[tb],\n",
    "                                compression=\"gzip\", compression_opts=9)\n",
    "        out_hdf5.create_dataset('stds/'+str(tb), data=stds[tb],\n",
    "                                compression=\"gzip\", compression_opts=9)\n",
    "    out_hdf5.create_dataset(\n",
    "        'train_targs_shuffles',\n",
    "        data = [[[r['train_target'] for r in rtb[1]]\n",
    "                 for rtb in res_shuffles[tb]]\n",
    "                for tb in time_bins],\n",
    "        compression=\"gzip\", compression_opts=9)\n",
    "    out_hdf5.close()\n",
    "    # make file read only to avoid accidental loss:\n",
    "    os.chmod(out_path_hdf5, 0o444)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007196856730011514 /scratch/cweidema/bootcamp/tmpdat/RAM/RAM_catFR/RAM_catFR1_power/encoding/hdf5_files_subj/R1374T_pow.hdf5\n",
      "0 / 26\n",
      "1 / 26\n",
      "2 / 26\n",
      "3 / 26\n",
      "4 / 26\n",
      "5 / 26\n",
      "6 / 26\n",
      "7 / 26\n",
      "8 / 26\n",
      "9 / 26\n",
      "10 / 26\n",
      "11 / 26\n",
      "12 / 26\n",
      "13 / 26\n",
      "14 / 26\n",
      "15 / 26\n",
      "16 / 26\n",
      "17 / 26\n",
      "18 / 26\n",
      "19 / 26\n",
      "20 / 26\n",
      "21 / 26\n",
      "22 / 26\n",
      "23 / 26\n",
      "24 / 26\n",
      "25 / 26\n",
      "Cannot create  /scratch/cweidema/bootcamp/tmpdat/RAM/RAM_FRcatFR/encoding/RAM_catFR1lolo_classifiers_allfreqs/RAM_catFR1lolo_classifiers_allfreqs_withshuffles/pickle_files/0.0007196856730011514/b'R1374T'_classifier_info.pickle\n",
      "0.0007196856730011514 /scratch/cweidema/bootcamp/tmpdat/RAM/RAM_catFR/RAM_catFR1_power/encoding/hdf5_files_subj/R1375C_pow.hdf5\n",
      "0 / 52\n",
      "1 / 52\n",
      "2 / 52\n",
      "3 / 52\n",
      "4 / 52\n",
      "5 / 52\n",
      "6 / 52\n",
      "7 / 52\n",
      "8 / 52\n",
      "9 / 52\n",
      "10 / 52\n",
      "11 / 52\n",
      "12 / 52\n",
      "13 / 52\n",
      "14 / 52\n",
      "15 / 52\n",
      "16 / 52\n",
      "17 / 52\n",
      "18 / 52\n",
      "19 / 52\n",
      "20 / 52\n",
      "21 / 52\n",
      "22 / 52\n",
      "23 / 52\n",
      "24 / 52\n",
      "25 / 52\n",
      "26 / 52\n",
      "27 / 52\n",
      "28 / 52\n",
      "29 / 52\n",
      "30 / 52\n",
      "31 / 52\n",
      "32 / 52\n",
      "33 / 52\n",
      "34 / 52\n",
      "35 / 52\n",
      "36 / 52\n",
      "37 / 52\n",
      "38 / 52\n",
      "39 / 52\n",
      "40 / 52\n",
      "41 / 52\n",
      "42 / 52\n",
      "43 / 52\n",
      "44 / 52\n",
      "45 / 52\n",
      "46 / 52\n",
      "47 / 52\n",
      "48 / 52\n",
      "49 / 52\n",
      "50 / 52\n",
      "51 / 52\n",
      "Cannot create  /scratch/cweidema/bootcamp/tmpdat/RAM/RAM_FRcatFR/encoding/RAM_catFR1lolo_classifiers_allfreqs/RAM_catFR1lolo_classifiers_allfreqs_withshuffles/pickle_files/0.0007196856730011514/b'R1375C'_classifier_info.pickle\n"
     ]
    }
   ],
   "source": [
    "for subjpath in subjpaths:\n",
    "    for c in cs:\n",
    "        print(c, subjpath)\n",
    "        lr_params['C'] = c\n",
    "        run_classification(subjpath, train_test_lr, train_test_lr_shuffles,\n",
    "                           lr_params, classifier_path, time_bins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTSA",
   "language": "python",
   "name": "ptsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
